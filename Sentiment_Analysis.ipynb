{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8kpu+42u+e5GYdOoO6Y8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilpocker/Sentiment-Analysis/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnHGakZOwKVZ",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Analysis of Amazon.com reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSMHxkbFw3XG",
        "colab_type": "text"
      },
      "source": [
        "**Sentiment Analysis:-** is the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive or negative.\n",
        "\n",
        "Here I have taken Amazon 'Health and Personal Care' product reviews, sourced from: - https://nijianmo.github.io/amazon/index.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yeIsRBEdNvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Required Imports\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow as tf \n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyUqJbspoj0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ1IEbtUApR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "92eae5fc-9b72-458d-c268-f24144e7d538"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz #download the data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-26 15:47:53--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85180885 (81M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Health_and_Personal_Care_5.json.gz’\n",
            "\n",
            "reviews_Health_and_ 100%[===================>]  81.23M  5.85MB/s    in 11s     \n",
            "\n",
            "2020-08-26 15:48:04 (7.41 MB/s) - ‘reviews_Health_and_Personal_Care_5.json.gz’ saved [85180885/85180885]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85InOMLw1BZb",
        "colab_type": "text"
      },
      "source": [
        "The downloaded data is an archive of the type *.gz(gzip)*, consisting of data in *json* format.\n",
        "The data needs to be extracted from the archive and loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWKxusQ7BehH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f762284e-b5a6-48e7-a99b-dc1bfb463850"
      },
      "source": [
        "### load the meta data\n",
        "\n",
        "data = []\n",
        "with gzip.open('reviews_Health_and_Personal_Care_5.json.gz') as f:\n",
        "    for l in f:\n",
        "        data.append(json.loads(l.strip()))\n",
        "    \n",
        "# total length of list, this number equals total number of products\n",
        "print(len(data))\n",
        "\n",
        "# first row of the list\n",
        "print(data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "346355\n",
            "{'reviewerID': 'ALC5GH8CAMAI7', 'asin': '159985130X', 'reviewerName': 'AnnN', 'helpful': [1, 1], 'reviewText': \"This is a great little gadget to have around.  We've already used it to look for splinters and a few other uses.  The light is great.  It's a handy size.  However, I do wish I'd bought one with a little higher magnification.\", 'overall': 5.0, 'summary': 'Handy little gadget', 'unixReviewTime': 1294185600, 'reviewTime': '01 5, 2011'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fAmSla1DTpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc52337a-9090-4b86-931e-b6b23144b653"
      },
      "source": [
        "# convert the obtained list into pandas dataframe\n",
        "\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "346355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33IRH83F14Pg",
        "colab_type": "text"
      },
      "source": [
        "The total length of the Dataframe is 346455, let us take a look at five rows of data, from 25 to 30."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZofiZJaD2DF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d3f1b789-b5cb-46a1-fe27-74ffb427edd4"
      },
      "source": [
        "df.iloc[25:30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>helpful</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>overall</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>reviewTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ADUTFUVJRHLKS</td>\n",
              "      <td>1933622865</td>\n",
              "      <td>Joel R. Wise</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>You have to be in the right spot to eliminate ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Helps with the small print</td>\n",
              "      <td>1395014400</td>\n",
              "      <td>03 17, 2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>A35W3JQYP0M655</td>\n",
              "      <td>1933622865</td>\n",
              "      <td>John Thomas... \"New England...USA\"</td>\n",
              "      <td>[68, 87]</td>\n",
              "      <td>I recently saw this at a local AC Moore store....</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...Wear Glasses And Forget This</td>\n",
              "      <td>1306627200</td>\n",
              "      <td>05 29, 2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>A26EXMDN188M0</td>\n",
              "      <td>1933622865</td>\n",
              "      <td>Lysan</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>Less helpful than a good magnifying glass. Jus...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Disappointed!</td>\n",
              "      <td>1374796800</td>\n",
              "      <td>07 26, 2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>A22DXDIYXPBVSP</td>\n",
              "      <td>1933622865</td>\n",
              "      <td>M. Heck</td>\n",
              "      <td>[1, 1]</td>\n",
              "      <td>Several reviews of similar items reported &amp;#34...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Nice and sturdy</td>\n",
              "      <td>1367625600</td>\n",
              "      <td>05 4, 2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>A2B3T42QBKDFFX</td>\n",
              "      <td>1933622865</td>\n",
              "      <td>Raymond Holderman \"raaymond1\"</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>This magnifies modestly well, is lightweight, ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>it's adequate</td>\n",
              "      <td>1397088000</td>\n",
              "      <td>04 10, 2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        reviewerID        asin  ... unixReviewTime   reviewTime\n",
              "25   ADUTFUVJRHLKS  1933622865  ...     1395014400  03 17, 2014\n",
              "26  A35W3JQYP0M655  1933622865  ...     1306627200  05 29, 2011\n",
              "27   A26EXMDN188M0  1933622865  ...     1374796800  07 26, 2013\n",
              "28  A22DXDIYXPBVSP  1933622865  ...     1367625600   05 4, 2013\n",
              "29  A2B3T42QBKDFFX  1933622865  ...     1397088000  04 10, 2014\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frac7h4X2avD",
        "colab_type": "text"
      },
      "source": [
        "There are a lot us unnecessary columns like *reviewerID, asin, reviewerName, etc*. Our interest mainly lies is the *reviewText* itself and the overall rating, which is the rating out of 5. \n",
        "\n",
        "So let us drop all the other columns from the Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJJaEw3jD7Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df.drop(['reviewerID','asin','reviewerName','unixReviewTime','reviewTime','helpful','summary'],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFp9chI_3AYD",
        "colab_type": "text"
      },
      "source": [
        "Since this is a binary classification into positive and negative reviews, we have to convert the overall rating (out of 5) into positive or negative. \n",
        "1 and 2 star reviews can be considered as negative and 3+ stars as positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yByvXcbITIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1['target'] = (df1['overall'] > 2).astype(int) #create new column 'target' which gives 1 if 'overall' is greater than 2, 0 otherwise."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ8uHlMSJBiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df1.drop('overall',axis = 1) #since we no longer need 'overall'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kf5sBQn3tDF",
        "colab_type": "text"
      },
      "source": [
        "Since the total length of the dataframe is huge, let us take only the last 50,000 for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqsRdMmJtAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df1[:50000]\n",
        "target = df2.pop('target') #store target variable (0/1) in another array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7vl9oscRkTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(df2['reviewText'].values, tf.string),\n",
        "            tf.cast(target.values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ") #convert the dataframe into Tensorflow dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImyAFQwz4Jw3",
        "colab_type": "text"
      },
      "source": [
        "Let us take a look at 5 entries in the dataset and its associated labels(target)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-HHSVOVZJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a50535a3-58f9-475b-b694-42f38f1e4f44"
      },
      "source": [
        "for feat, targ in dataset.take(5):\n",
        "  print ('Features: {}, Target: {}'.format(feat, targ))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: b\"This is a great little gadget to have around.  We've already used it to look for splinters and a few other uses.  The light is great.  It's a handy size.  However, I do wish I'd bought one with a little higher magnification.\", Target: 1\n",
            "Features: b'I would recommend this for a travel magnifier for the occasional reading.I had read on another review about a magnifier having a problem with the light coming on. I did find that this one appeared to be DOA out of the box. But, after opening & shutting the viewer to turn on & off the light, the light began to come on. After several times of doing this, the light appears to be coming on all the time.It is small, but for taking it someplace & reading things like a menu in a dark corner of a restaurant, this is great.', Target: 1\n",
            "Features: b\"What I liked was the quality of the lens and the built in light.  Then lens had no discernable distortion anywhere.  It magnified everything evenly without the ripples and  distortion that I've seen with other low cost magnifiers.  This light is a nice touch and easy to use.  If you want it on just pull the lens out a bit.  It is focused very close to the center of what you will be look at and provides nice, even coverage.What I didn't like was the brightness (actually dimmness) of the light and where it is focused.  LEDs can be lots brighter, I know as I've seen them.  Also, the light focuses at the center of you field of view but only when the lens is too close to be focused properly.Bottom line is this is a good value for a magnifier and could have been made great with better quality control.BTW, I feel that honest, effective reviews can take the place of first-hand experiences that are lacking in online shopping. I've always appreciated the help I've received from other reviewers and work hard to return the favor as best as I can.  I hope you found this review helpful and if there was anything you thought was lacking or unclear leave a comment and I'll do what I can to fix it.\", Target: 1\n",
            "Features: b\"Love the Great point light pocket magnifier!  works great, especially if you forget your glasses and can't read the menu when you are out for dinner.... the light is a bonus in a dark restaurant, too.  I would have given 5 stars if it came with a case that covers the glass.  The clear plastic case it comes in is a little tacky and I'd be willing to pay a little more for a nice case to protec the lens.\", Target: 1\n",
            "Features: b'This is very nice. You pull out on the magnifier when you want the light to come on, then slide it back in. I would recommend buying this if you need something with a light that you can easily put in your pocket or purse.', Target: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQxyM3RM4ZTT",
        "colab_type": "text"
      },
      "source": [
        "Next, let us split the data into train, test, and validation. Let the split be 70% train, 15% validation and 15%test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgApux3we34Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_SIZE = len(dataset)\n",
        "\n",
        "\n",
        "train_size = int(0.7 * DATASET_SIZE)\n",
        "val_size = int(0.15 * DATASET_SIZE)\n",
        "test_size = int(0.15 * DATASET_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh4GvcPfjMWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_train_dataset = dataset.take(train_size)\n",
        "raw_test_dataset = dataset.skip(train_size)\n",
        "raw_val_dataset = raw_test_dataset.skip(val_size)\n",
        "raw_test_dataset = raw_test_dataset.take(test_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqvOVMGV44C-",
        "colab_type": "text"
      },
      "source": [
        "Next step is to convert the reviews data and vectorise it (map each word into integers) for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGVhdkx7nebF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000 #total number of words\n",
        "sequence_length = 250 \n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsoPnB0En-LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = dataset.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSAiVWY4D-Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_9SkbMA5VN1",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the mapped integer to the corresponding word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__FpPv1tHbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b2784645-496f-4f1f-c2dd-45e98c3c5c46"
      },
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1287 --->  safety\n",
            " 313 --->  face\n",
            "Vocabulary size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ay8UV55eWm",
        "colab_type": "text"
      },
      "source": [
        "Now vectorise all three sets seperately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZkPegyGy1JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = raw_train_dataset.map(vectorize_text)\n",
        "val_ds = raw_val_dataset.map(vectorize_text)\n",
        "test_ds = raw_test_dataset.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILZv1n4P7RtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = train_ds.map(lambda x_text, x_label: (x_text, tf.expand_dims(x_label, -1))) #to match dimensions of 'target' while fitting model.\n",
        "val_ds = val_ds.map(lambda x_text, x_label: (x_text, tf.expand_dims(x_label, -1)))\n",
        "test_ds = test_ds.map(lambda x_text, x_label: (x_text, tf.expand_dims(x_label, -1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZOzloGXzA-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kx5I7ANzOl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0GWVh6S57Ww",
        "colab_type": "text"
      },
      "source": [
        "Design the model, it consists of an embedding layer, a dropout after it, a bidirection LSTM, A dense layer with 32 units, another dropout layer and finally an output Layer with 1 unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XbqNb2F10EA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "418ba34a-1b97-415c-de64-f69abd5fd3cf"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "  layers.Dense(32),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary() #view model summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 32)                4224      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 165,329\n",
            "Trainable params: 165,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWFxLvzO6VKo",
        "colab_type": "text"
      },
      "source": [
        "Compile the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0CHDcRm2HCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h0gDSa15bAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "ac4c4174-323c-47f7-91fa-4506c2748b37"
      },
      "source": [
        "epochs = 8\n",
        "batch_size = 32\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[callback]) #train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "35000/35000 [==============================] - 707s 20ms/step - loss: 0.2799 - binary_accuracy: 0.9021 - val_loss: 0.3148 - val_binary_accuracy: 0.8896\n",
            "Epoch 2/8\n",
            "35000/35000 [==============================] - 704s 20ms/step - loss: 0.2215 - binary_accuracy: 0.9138 - val_loss: 0.2633 - val_binary_accuracy: 0.8969\n",
            "Epoch 3/8\n",
            "35000/35000 [==============================] - 690s 20ms/step - loss: 0.1901 - binary_accuracy: 0.9278 - val_loss: 0.2677 - val_binary_accuracy: 0.9040\n",
            "Epoch 4/8\n",
            "35000/35000 [==============================] - 681s 19ms/step - loss: 0.1709 - binary_accuracy: 0.9351 - val_loss: 0.2799 - val_binary_accuracy: 0.9049\n",
            "Epoch 5/8\n",
            "35000/35000 [==============================] - 683s 20ms/step - loss: 0.1547 - binary_accuracy: 0.9416 - val_loss: 0.2944 - val_binary_accuracy: 0.9068\n",
            "Epoch 6/8\n",
            "35000/35000 [==============================] - 697s 20ms/step - loss: 0.1450 - binary_accuracy: 0.9457 - val_loss: 0.2971 - val_binary_accuracy: 0.9039\n",
            "Epoch 7/8\n",
            "35000/35000 [==============================] - 698s 20ms/step - loss: 0.1357 - binary_accuracy: 0.9487 - val_loss: 0.2896 - val_binary_accuracy: 0.9071\n",
            "Epoch 8/8\n",
            "35000/35000 [==============================] - 703s 20ms/step - loss: 0.1307 - binary_accuracy: 0.9521 - val_loss: 0.3162 - val_binary_accuracy: 0.9051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ayHtDe6jK7",
        "colab_type": "text"
      },
      "source": [
        "This should be around 95% accuracy on the train set and 90% on the validation set, which means it is overfitting and exhibits variance. Let us now test it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuenhSd35fI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d141f28d-cbad-43bc-b7ed-2809d381f792"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7500/7500 [==============================] - 64s 8ms/step - loss: 0.3002 - binary_accuracy: 0.9112\n",
            "Loss:  0.3001821041107178\n",
            "Accuracy:  0.9111999869346619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjm2xqOw7JLK",
        "colab_type": "text"
      },
      "source": [
        "On the test set it shows an accuracy of 91% which is decent. By removing the overfitting, improving and optimsing the model, the accuracy could be higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP5xT6ST7Fp1",
        "colab_type": "text"
      },
      "source": [
        "Finally, save the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV6ZOtZ9eO-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}